import csv, time
import numpy as np
from utils import data_utils, plot_utils
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
from keras.constraints import maxnorm

dataset = {}
dataset['io'] = {}
dataset['io']['relative path'] = '.'
dataset['io']['directory'] = 'data_mat'
dataset['io']['extension'] = '.mat'
dataset['io']['train fname'] = 'data_batch'
dataset['io']['test fname'] = 'test_data'

dataset['image'] = {}
dataset['image']['shape'] = (32, 32, 3)
dataset['image']['unrolled'] = 32*32*3

dataset['num classes'] = 3
dataset['num train examples'] = 12000
dataset['num test examples'] = 3000
dataset['validation split'] = 8 # 12000 / 8 = 1500

(train, valid, test) = data_utils.load_data(dataset)

data_augmentation = False
nb_classes = 3
epochs = 50
batch=64
model = Sequential()


model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))
model.add(Activation('relu'))
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

# Compile model
lrate = 0.01
decay = lrate/epochs
sgd = SGD(lr=lrate, momentum=0.9, decay=1e-6, nesterov=False)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])


if not data_augmentation:
    print('Not using data augmentation.')
    res = model.fit(train['x'], train['y'],
              batch_size=batch,
              epochs=epochs,
              validation_data=(valid['x'], valid['y']),
              shuffle=True)
    #model.fit(train['x'], train['y'], epochs=epochs, batch_size=batch,validation_data = (valid['x'], valid['y']))
    score = model.evaluate(train['x'], train['y'])
else:
    print('Using real-time data augmentation.')
    # This will do preprocessing and realtime data augmentation:
    datagen = ImageDataGenerator(
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True)
    # Compute quantities required for feature-wise normalization
    # (std, mean, and principal components if ZCA whitening is applied).
    datagen.fit(x_train)

    # Fit the model on the batches generated by datagen.flow().
    res = model.fit_generator(datagen.flow(train['x'], train['y'],
                                     batch_size=batch),
                        steps_per_epoch=x_train.shape[0] // batch,
                        epochs=epochs,
                        validation_data=(valid['x'], valid['y']))


y_pred_test = model.predict_classes(test['x'], verbose=0)
print(y_pred_test)
y_pred_test = np.array(y_pred_test)

with open('out_1.csv','wb') as myfile:
    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)
    for pred in y_pred_test:
        wr.writerow([pred])

plot_utils.plot_model_history(res)
